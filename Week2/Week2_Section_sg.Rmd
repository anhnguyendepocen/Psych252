Section Week 2 
========================================================

*1. Standard Deviation vs. Standard Error*
*2. Centering data and Standardizing data vs. "Normalizing" Data*
*3. T-tests and the null hypothesis*

## Standard Deviation vs. Standard Error

Standard Deviation

```{r calculating standard deviation}
## entering data -- let's say that we want to calculate the average temperature for palo alto.  so on 6 days we observe the temperature

d <- c(80, 76, 81, 72, 68, 76) 

## calculating mean
m <- mean(d)
print(m)

## calculating number of observations
n <- length(d)
print(n)

## calculating standard deviation
sqrt(var(d))

# or

s <- sd(d)
print(s)
```

Standard error

```{r calculating standard error}
se <- s/ n^.5
print(se)

# this also works:
se = s/sqrt(n)
print(se)
```

Now we can use the standard error of the mean to calculate a confidence interval for the mean!

```{r confidence intervals}
## calculating 95% confidence interval
ci95 <- se * 1.96  ## why 1.96? Critical value for 95% CI
print(ci95)

# using R to get z scores, given p
alpha = 0.05
z_critical = qnorm(alpha/2, lower.tail=FALSE)
print(z_critical)

## adding and subtracting it from mean
lowerbound = m - ci95
upperbound = m + ci95
```

So the confidence interval for the mean is from 71.58 to 79.41, which means that the mean temperature in Palo Alto falls within this interval.

## Centering vs. Standardizing Data

So let's say we have a variable "i" and want to either center or standardize it.

```{r setting x}
## we have a data set with 4 variables
dn <- c(3,4,15,8)
i <- 3
i2 <- 4
i3 <- 15
i4 <- 8

## find mean and standard deviation for calculations
mn <- mean(dn)
print(mn)
sdn <- sd(dn)
print(sdn)
```

Centering - subtracting mean from values

```{r}
ic = (i - mn)
print(ic)

i2c = (i2 - mn)
print(i2c)

i3c = (i3 - mn)
print(i3c)

i4c = (i4 - mn)
print(i4c)

## combine values

centered <-c (ic, i2c, i3c, i4c)
print(centered)
plot(centered)
```

That took a while... can we do it using matrix/vector operations??

```{r}
icf = dn-mn

print(icf)
plot(icf)
```

Note, there's a helpful function that R has called `scale` -- we can use this to automatically center our data, like this:
```{r}
centered_data = scale(dn , scale = FALSE)

print(centered_data)
plot(centered_data)
```
n.b., if you just want to center the data, set **scale = FALSE**!



Now what is our mean and standard deviation?

```{r}
mean(icf)
sd(icf)
```

Standardization - subtracting mean, dividing by standard deviation

```{r}
is = (i - mn)/sdn
print(is)

standard <- (dn-mn)/sdn

print(standard)
plot(standard)
```

Now let's take a look at our mean and sd...

```{r}
mean(standard)
sd(standard)
```

You can do this with the scale function too!
```{r}
standardized_data = scale(dn, scale=TRUE)

print(standardized_data)
plot(standardized_data)
```



How do you do this on a dataset in R?

```{r data entry}
# setwd
d2 = read.csv('dataset_scale.csv')
```

We're looking at a dataset of height and salary.  Let's check it out...

```{r useful functions for looking at data}
summary(d2)
head(d2)
tail(d2)
str(d2$height)
```

We're working with height here, so it doesn't make sense to talk about expected salary when height is set to 0 - the mean is more interpretable.  So we center!

Center the data!

```{r}
# before centering
with(d2, plot(height, salary))


d2$heightc = with(d2,scale(height, center = TRUE, scale = FALSE))
head(d2$heightc)
with(d2, plot(heightc, salary))
abline(v=0, col='red', lty = 2)
```

Create standardized scores!

```{r}
d2$heights = with(d2, scale(height, center = TRUE, scale = TRUE))
head(d2$heights)
with(d2, plot(heights, salary))
abline(v=0, col='red', lty = 2)
```

Now let's reexamine our data set...

```{r}
summary(d2)
sd(d2$heightc)
sd(d2$heights)
```

Note that the means are 0, and the differences in standard deviation.

Does this affect our output?

```{r tests with normalized variables}
# non normalized
rs1 = lm(d2$salary~d2$height)
summary(rs1)
plot(d2$height,d2$salary)
lines(abline(rs1,col='red'))

# centered
rs2 = lm(d2$salary~d2$heightc)
summary(rs2)

plot(d2$heightc,d2$salary)
lines(abline(rs2,col='red'))

# standardized
rs3 = lm(d2$salary~d2$heights)
summary(rs3)
plot(d2$heights,d2$salary)
lines(abline(rs3,col='red'))
```

Note the change in estimates for the last part, but that significance never changes.
Note also the intercept.

### Differences between normalizing data and normal distributions

Standardizing data doesn't fix problems of skewness.  Let's take a look at a skewed distribution

```{r}
x0 = c(1:3, 5, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9)
plot(x0)
hist(x0)

msk = mean(x0)
print(msk)

sdsk = sd(x0)
print(sdsk)

## load psych library, test for skewness
install.packages('psych')
library(psych)
skew(x0)
```

Now let's standardize...

```{r standardizing skewed distributions}

skewst = (x0-msk)/sdsk

print(skewst)
hist(skewst)

## now we can see that the mean and SD are at 0 and 1
mean(skewst)
sd(skewst)

## BUT the plot looks exactly the same -- the data is still skewed!
skew(skewst)
```

## T-tests and the Null Hypothesis

Let's generate a distribution that represents the null.

```{r t test simulation null}

## generating random data

group = rnorm(100, mean=0, sd=1)
```

Let's plot this distribution.

```{r plot null}
hist(group)
```

Let's do a t.test -- is this different from 0?
```{r t test}
t.test(group)
```

Taking samples from a distribution
```{r one sample}
samp1 = sample(group, size=10, replace=F)
t.stat = t.test(samp1)$statistic
print(t.stat)
```

If we took 1000 samples from this distribution:

```{r sampling null}
scores = group
R = 1000                                   
t.values = numeric(R)  

for (i in 1:R) {
groups = sample(scores, size=100, replace=T)
t.values[i] = t.test(groups)$statistic
}
```

Plotting t values!

```{r plot t-values}
hist(t.values, breaks=20)
```

Some t-values that are outside of the range!

## Power

```{r calculating power null}
m <- 5
sd <- 2
n <- 20
alpha = 0.05
       
sterr = sd/sqrt(n)

tstat = qt(alpha/2, df=n-1, lower.tail=FALSE); print(tstat)

merror = tstat*sterr; print(merror)

left = m - merror
right = m + merror
print(left); print(right)
```

```{r mean CI}
t = m + 1.5; print(t)
tleft = (left-t) / (s/sqrt(n))
tright = (right-t) / (s/sqrt(n))
print(tleft); print(tright)
p = pt(tright, df = n-1) - pt(tleft, df = n-1)
print(p)
```

Doing this same calculation in one line:

```{r power.t.test}
power.t.test(n=n,delta=1.5,sd=sd,sig.level=0.05, type="one.sample",alternative="two.sided")
```

What else can this function do?

```{r power.t.test examples}
power.t.test(delta=1.5,sd=sd,sig.level=0.05,power=.8, type="one.sample",alternative="two.sided") #80% power

power.t.test(delta=1.5,sd=sd,sig.level=0.05,power=.9, type="one.sample",alternative="two.sided") #90% power
```
